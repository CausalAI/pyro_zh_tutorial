{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# SVI Part I: 随机变分推断基础\n",
    "\n",
    "\n",
    "Pyro在设计时特别注意支持随机变分推断作为通用推断算法。让我们看看我们使用Pyro进行变分推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Table of Contents**\n",
    "\n",
    "- [SVI数学基础](#SVI数学基础)\n",
    "- [近似后验的guide](#近似后验的guide)\n",
    "- [目标函数ELBO](#目标函数ELBO)\n",
    "- [SVI Class](#SVI-类)\n",
    "- [优化器](#optimizers)\n",
    "- [端对端例子](#端对端例子)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "based on the data and our prior belief, the fairness of the coin is 0.531 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "# Hint for the rest of this tutorial\n",
    "# 完成本章学习之后，您将理解如下的程序\n",
    "import math, os, torch, pyro\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "data = []\n",
    "data.extend([torch.tensor(1.0) for _ in range(6)])\n",
    "data.extend([torch.tensor(0.0) for _ in range(4)])\n",
    "\n",
    "def model(data):\n",
    "    alpha0, beta0 = torch.tensor(10.0), torch.tensor(10.0)\n",
    "    theta = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    for i in range(len(data)):\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(theta), obs=data[i])\n",
    "def guide(data):\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "    if step % 100 == 0:\n",
    "        print('.', end='')\n",
    "\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 随机变分推断数学背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们将假设我们已经在Pyro中定义了模型（有关如何完成此操作的更多详细信息，请参见 [Intro Part I](intro_part_i.ipynb)）。简单回顾一下，模型就是一个带参数的随机函数 `model(*args, **kwargs)`. 模型的每个部分都对应于某个 Pyro 语句:\n",
    "\n",
    "1. 观测变量 $\\Longleftrightarrow$ `pyro.sample` with the `obs` argument\n",
    "2. 潜变量 $\\Longleftrightarrow$ `pyro.sample`\n",
    "3. 模型参数 $\\Longleftrightarrow$ `pyro.param`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们来看看变分推断背后的数学形式。 一个模型有观测变量 ${\\bf x}$, 潜变量 ${\\bf z}$ 和参数 $\\theta$. 联合概率密度如下： \n",
    "\n",
    "$$p_{\\theta}({\\bf x}, {\\bf z}) = p_{\\theta}({\\bf x}|{\\bf z}) p_{\\theta}({\\bf z})$$\n",
    "\n",
    "\n",
    "我们假定组成 $p_{\\theta}({\\bf x}, {\\bf z})$ 的每个概率分布 $p_i$ 具备以下的性质：\n",
    "\n",
    "1. 可以直接对它抽样 \n",
    "2. 可以计算某点处的概率密度\n",
    "3. 关于模型参数 $\\theta$ 可微"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<font size=4> 一个直接的问题是：如何学习模型 $p_{\\theta}({\\bf x}, {\\bf z})$? </font>\n",
    "\n",
    "在这种情况下，我们通过最大化对数证据(log evidence) 这个标准来学习一个好的模型, i.e. we want to find the value of $\\theta$ given by\n",
    "\n",
    "$$\\theta_{\\rm{max}} = \\underset{\\theta}{\\operatorname{argmax}} \\log p_{\\theta}({\\bf x})$$\n",
    "\n",
    "其中对数证据 $\\log p_{\\theta}({\\bf x})$ 满足：\n",
    "\n",
    "$$\\log p_{\\theta}({\\bf x}) = \\log \\int_{\\bf z}\\! \\; p_{\\theta}({\\bf x}, {\\bf z}) d{\\bf z}$$\n",
    "\n",
    "\n",
    "在一般情况下，这是一个 doubly difficult problem. 这是因为(even for a fixed $\\theta$) 对潜变量 $\\bf z$ 上的积分通常难以计算的。此外，即使我们知道如何计算所有 $\\theta$ 值的对数证据，也就是说对数证据是一个以参数 $\\theta$ 为自变量的函数，最大化它通常将是一个困难的非凸优化问题。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "除了找到 $\\theta_{\\rm{max}}$ 之外，我们还要计算潜变量 $\\bf z$ 的后验分布：\n",
    "\n",
    "$$ p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x}) = \\frac{p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z})}{\n",
    "\\int_{\\bf z} \\! \\; p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z})d{\\bf z} } $$\n",
    "\n",
    "请注意，此表达式的分母 is the evidence(通常不可计算). 变分推断提供一种方案用于求解$\\theta_{\\rm{max}}$ 和计算一个近似后验分布 $p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x})$. \n",
    "\n",
    "总的来说，我们使用极大似然估计去求的 $\\theta_{max}$ 的思路会遇到很多麻烦。变分推断的目的是一方面估计出来联合分布的参数(也就是模型参数，得到生成模型)，另外一个方面是得到后验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 近似后验的guide\n",
    "\n",
    "\n",
    "基本的想法是用一个带参指导分布 $q_\\phi(z)$ ($\\phi$ 被叫做变分参数) 来近似真实后验分布 $p_\\theta(z|x)$, 其中 $q$ 被称作变分分布 (variational distribution) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "指导分布 `guide()` 和模型分布一样, 它是一个包含 `pyro.sample` 和 `pyro.param` 语句的随机函数. 但是指导分布不包含任何观测数据, since the guide needs to be a properly normalized distribution. 注意在 Pyro 中，两个可调用对象 `model()` 和  `guide()` 必须具有相同的输入参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "因为指导分布是潜变量后验分布 $p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x})$ 的近似, 那么指导分布 `guide()` 需要提供模型中所有潜变量的有效联合概率密度。 模型分布和指导分布中抽样语句 `pyro.sample()` 的变量名字必须对齐。 确切地说，如果 `model()` 包含随机变量 `z_1`\n",
    "\n",
    "```python\n",
    "def model():\n",
    "    pyro.sample(\"z_1\", ...)\n",
    "```\n",
    "\n",
    "那么 guide 具备有对应的 `sample` statement\n",
    "\n",
    "```python\n",
    "def guide():\n",
    "    pyro.sample(\"z_1\", ...)\n",
    "```\n",
    "\n",
    "两种情况下使用的分布可以不同，但是名称必须一一对应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "一旦指定了指导分布 `guide()` （我们在下面提供了一些明确的示例），我们就可以进行推断了。学习将就是一个优化问题 where each iteration of training takes a step in $\\theta-\\phi$ space that moves the guide closer to the exact posterior. 为此，我们需要定义适当的目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**注**：\n",
    "\n",
    "- 因为 `model = primitive r.v.(观测变量 + 潜变量) + deterministic function` 是一个具备观测变量的分布, 而 `guide` 给定观测变量潜变量后验分布的近似分布。\n",
    "- 在变分自编码器中，用来近似后验分布的 `guide` $q_\\phi(z)$ 是局部的，意味着对一个每个样本 $x_i$, `latent r.v.` 的后验分布 $q_\\phi(z|x_i)$ 都不相同，我们需要学习与样本个数相同数量的后验分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 目标函数ELBO\n",
    "\n",
    "(定义目标函数) A simple derivation (for example see reference [1]) yields what we're after: the evidence lower bound (ELBO). The ELBO, which is a function of both $\\theta$ and $\\phi$, is defined as an expectation w.r.t. to samples from the guide:\n",
    "\n",
    "$${\\rm ELBO} \\equiv \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [ \n",
    "\\log p_{\\theta}({\\bf x}, {\\bf z}) - \\log q_{\\phi}({\\bf z})\n",
    "\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "根据假设我们可以计算上式中期望内部对数概率 (也就是 $\\log p_{\\theta}({\\bf x}, {\\bf z})$ 和 $\\log q_{\\phi}({\\bf z})$). 因为指导分布是一个可以从中采样的参数分布, 所以我们可以计算 ELBO 的蒙特卡洛估计. 至关重要的是 ELBO 为对数证据的下限，即对于所有的 $\\theta$ 和 $\\phi$，我们都有\n",
    "\n",
    "$$\\log p_{\\theta}({\\bf x}) \\ge {\\rm ELBO}$$\n",
    "\n",
    "\n",
    "因此，如果我们采取(stochastic) gradient steps 最大化 ELBO，那么我们也会 pushing the log evidence higher (in expectation). 此外，可以证明ELBO 和对数证据之间的差就是 `guide` 和潜变量后验分布之间的KL散度： \n",
    "\n",
    "$$ \\log p_{\\theta}({\\bf x}) - {\\rm ELBO} = \n",
    "\\rm{KL}\\!\\left( q_{\\phi}({\\bf z}) \\lVert p_{\\theta}({\\bf z} | {\\bf x}) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "KL 散度是两个分布之间 “相似性” 的一个非负度量。所以对每个固定的 $\\theta$, as we take steps in $\\phi$ space that increase the ELBO, we decrease the KL divergence between the guide and the posterior, 也就是说，我们让 `guide` 更加接近后验分布了。 而不固定的 $\\theta$时， we take gradient steps in both $\\theta$ and $\\phi$ space simultaneously so that the guide and model play chase, with the guide tracking a moving posterior $\\log p_{\\theta}({\\bf z} | {\\bf x})$. 或许有些令人惊讶, despite the moving target, 对很多不同的问题来说这个优化问题可以解决 (to a suitable level of approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "简单来说，ELBO 是 SVI 最常见的目标函数，因为 \n",
    "\n",
    "$${\\rm ELBO} = \\log p_{\\theta}({\\bf x}) -  \\rm{KL}\\!\\left( q_{\\phi}({\\bf z}) \\lVert p_{\\theta}({\\bf z}|{\\bf x}) \\right) \\leq \\log p_{\\theta}({\\bf x})$$\n",
    "\n",
    "是 $\\log p_{\\theta}({\\bf x})$ 的下界，所以它被叫做证据下界(evidence lower bound)。其中KL散度的定义：\n",
    "\n",
    "$$KL(p(x)|q(x)) = E_{p(x)}\\log \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "See [KL散度(Kullback–Leibler divergence)非负性证明\n",
    "](https://blog.csdn.net/root_clive/article/details/103941412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "在总体思想上，变分推断很容易：我们所需要做的就是定义一个 `guide()` 并计算 ELBO 关于模型和指导分布参数的梯度。实际上，计算一般模型分布和指导分布的参数梯度会有一些技术难度 (see the tutorial [SVI Part III](svi_part_iii.ipynb) for a discussion). 就本教程而言, 让我们考虑一个已解决的问题，并看看如何使用 Pyro 进行变分推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## `SVI` 类\n",
    "\n",
    "在 Pyro 中变分推断被封装在 `SVI` 的类中，目前只支持 ELBO 目标函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "构建一个SVI对象，用户需要指定三个输入： model,  guide 和 optimizer. 我们已经在上面讨论了模型和指导分布，并且将在下一节详细讨论优化器，因此我们现在假设手头已经有了这三个要素。To construct an instance of `SVI` that will do optimization via the ELBO objective, the user writes\n",
    "\n",
    "```python\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " `SVI` 对象有两个方法, `step()` 和 `evaluate_loss()`, that encapsulate the logic for variational learning and evaluation:\n",
    "\n",
    "1. 该方法 `step()` 对其模型和指导分布中参数进行一步梯度下降，并且返回估计的损失函数 (i.e. minus the ELBO). If provided, the arguments to `step()` are piped to `model()` and `guide()`. \n",
    "\n",
    "2. 该方法 `evaluate_loss()` 返回估计的损失函数，但是不进行梯度下降. Just like for `step()`, if provided, arguments to `evaluate_loss()` are piped to `model()` and `guide()`.\n",
    "\n",
    "\n",
    "对于损失函数为 ELBO 的情况，这两种方法都有可选参数 `num_particles`, 该参数表示样本数 used to compute the loss (in the case of `evaluate_loss`) and the loss and gradient (in the case of `step`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## optimizers\n",
    "\n",
    "在Pyro中，model 和 guide 可以是满足如下条件的任意随机函数:\n",
    "\n",
    "1. `guide` 不能包含 `model` 中任何具有参数 `obs` 的 `pyro.sample` 语句.\n",
    "2. `model` 和 `guide` 具备相同的输入参数(call signiture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "(动态生成的潜变量和参数问题) This presents some challenges because it means that different executions of `model()` and `guide()` may have quite different behavior, with e.g. certain latent random variables and parameters only appearing some of the time. **Indeed parameters may be created dynamically during the course of inference.** 也就是说 the space we're doing optimization over, which is parameterized by $\\theta$ and $\\phi$, can grow and change dynamically.\n",
    "\n",
    "\n",
    "In order to support this behavior, Pyro 需要为学习过程中新出现的参数动态的生成一个优化器. 幸运的是, PyTorch有一个轻量级的优化库 (see [torch.optim](http://pytorch.org/docs/master/optim.html)) that  can easily be repurposed for the dynamic case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "All of this is controlled by the `optim.PyroOptim` class, which is basically a thin wrapper around PyTorch optimizers. `PyroOptim` 有两个输入参数: a constructor for PyTorch optimizers `optim_constructor` and a specification of the optimizer arguments `optim_args`. 总的来说, 就是在优化的过程中, 一旦某个新的参数产生， `optim_constructor` 就会初始化一个指定类型的优化器 with arguments given by `optim_args`.\n",
    "\n",
    "Most users will probably not interact with `PyroOptim` directly and will instead interact with the aliases defined in `optim/__init__.py`. Let's see how that goes. \n",
    "\n",
    "有两种方法可以指定优化器参数。简单的情况是, 使用一个指定参数的固定字典 `optim_args` 来初始化一个 PyTorch optimizers for _all_ the parameters:\n",
    "\n",
    "```python\n",
    "from pyro.optim import Adam\n",
    "\n",
    "adam_params = {\"lr\": 0.005, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "第二种指定参数的方法可以实现更精细的控制。这里用户需要指定由 Pyro 唤醒的一个可调用对象，该对象将在为新生成的参数创建优化器。 该可调用对象必须有如下两个输入:\n",
    "\n",
    "1. `module_name`: 包含参数的模块的的 Pyro name, if any\n",
    "2. `param_name`: 参数的 Pyro name \n",
    "\n",
    "This gives the user the ability to, for example, customize learning rates for different parameters. For an example where this sort of level of control is useful, see the [discussion of baselines](svi_part_iii.ipynb). 下面用一个简单的例子说明这个API. \n",
    "\n",
    "```python\n",
    "from pyro.optim import Adam\n",
    "\n",
    "def per_param_callable(module_name, param_name):\n",
    "    # 该函数与 module_name 无关，让我疑惑\n",
    "    if param_name == 'my_special_parameter':\n",
    "        return {\"lr\": 0.010}\n",
    "    else:\n",
    "        return {\"lr\": 0.001}\n",
    "\n",
    "optimizer = Adam(per_param_callable)\n",
    "```\n",
    "这相当于告诉 Pyro 将参数 `my_special_parameter` 的学习速率设为 `0.010`，并将所有其他参数的学习速率设为`0.001`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 端对端例子\n",
    "\n",
    "\n",
    "我们以一个简单的例子结束。您已获得两面硬币。您想确定硬币是否公平，即硬币以相同的频率出现正面或背面. \n",
    "\n",
    "(先验分布是 $\\rm{Beta}(10,10)$) You have a prior belief of $\\rm{Beta}(10,10)$ about the likely fairness of the coin based on two observations:\n",
    "\n",
    "- it's a standard quarter issued by the US Mint\n",
    "- it's a bit banged up from years of use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "<center><figure><img src=\"_static/img/beta.png\" style=\"width: 300px;\"><figcaption> <font size=\"-1\"><b>Figure 1</b>: Beta分布编码了我们对硬币公平性的先验信念。 </font></figcaption></figure></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们的模型是:\n",
    "\n",
    "$$\n",
    "\\theta \\sim Beta(10, 10) \\\\\n",
    "\\text{Measurement} |\\theta \\sim B(0, 1; \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "假设我们做了若干次试验，并且已经将 Measurements 收集在列表 `data` 中，则相应的 Pyro 模型是\n",
    "\n",
    "```python\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample theta from the beta prior\n",
    "    theta = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli \n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(theta), obs=data[i])\n",
    "```\n",
    "\n",
    "这里我们有一个唯一的潜变量 `'latent_fairness'`, 它的分布是 $\\rm{Beta}(10, 10)$. Conditioned on that random variable, we observe each of the datapoints using a bernoulli likelihood. 请注意，每个观测都在 Pyro 中分配了唯一的名称."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们的下一个任务是定义对应的指导分布 `guide()`，即为潜在随机变量 $\\theta$ 分配适当的变分分布。 A simple choice is to use another beta distribution parameterized by two trainable parameters $\\alpha_q$ and $\\beta_q$. Actually, in this particular case this is the 'right' choice, since conjugacy of the bernoulli and beta distributions means that the exact posterior is a beta distribution. In Pyro we write:\n",
    "\n",
    "```python\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), \n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), \n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "这里有几件事情需要注意：\n",
    "\n",
    "- 模型分布和指导分布中潜变量的名字必须严格一致。\n",
    "- `model(data)` 和 `guide(data)` 具备相同的输入。\n",
    "- 变分参数 `torch.tensor`. 而 `pyro.param` 中默认 `requires_grad=True`.\n",
    "- 我们使用 `constraint=constraints.positive` 来实现 `alpha_q` 和 `beta_q` 在优化过程中的非负约束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "现在我们可以进行随机变分推断了。\n",
    "\n",
    "```python\n",
    "# set up the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 5000\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "```    \n",
    "\n",
    "注意 `step()` 方法中传入的 `data`, 它会被传入 `model()` 和 `guide()`. The only thing we're missing at this point is some data. So let's create some data and assemble all the code snippets above into a complete script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "based on the data and our prior belief, the fairness of the coin is 0.535 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "n_steps = 2 if smoke_test else 2000\n",
    "\n",
    "# enable validation (e.g. validate parameters of distributions)\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    theta = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(theta), obs=data[i])\n",
    "\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0. \n",
    "    # - because we invoke constraints.positive, the optimizer \n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), \n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), \n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "    if step % 100 == 0:\n",
    "        print('.', end='')\n",
    "\n",
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "(可以看出结果接近精确后验推断的值) This estimate is to be compared to the exact posterior mean, which in this case is given by $16/30 = 0.5\\bar{3}$.\n",
    "Note that the final estimate of the fairness of the coin is in between the the fairness preferred by the prior (namely $0.50$) and the fairness suggested by the raw empirical frequencies ($6/10 = 0.60$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 参考文献\n",
    "\n",
    "[1] `Automated Variational Inference in Probabilistic Programming`,\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "David Wingate, Theo Weber\n",
    "\n",
    "[2] `Black Box Variational Inference`,<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Rajesh Ranganath, Sean Gerrish, David M. Blei\n",
    "\n",
    "[3] `Auto-Encoding Variational Bayes`,<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Diederik P Kingma, Max Welling"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
